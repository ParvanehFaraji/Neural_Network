{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParvanehFaraji/Neural_Network/blob/main/NN_Project_Parvaneh_Faraji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbO-ttuNX0Bo",
        "outputId": "d50fd47b-9b60-4d02-a5a6-3774c93c053f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch [1/10], Loss: 0.5144\n",
            "Epoch [2/10], Loss: 0.3814\n",
            "Epoch [3/10], Loss: 0.3383\n",
            "Epoch [4/10], Loss: 0.3173\n",
            "Epoch [5/10], Loss: 0.2999\n",
            "Epoch [6/10], Loss: 0.2825\n",
            "Epoch [7/10], Loss: 0.2705\n",
            "Epoch [8/10], Loss: 0.2570\n",
            "Epoch [9/10], Loss: 0.2486\n",
            "Epoch [10/10], Loss: 0.2386\n",
            "Test Accuracy: 87.12%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(nn.ReLU())  # Default activation function\n",
        "            in_features = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(in_features, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 28 * 28\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = MLP(input_size, hidden_sizes, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "print(\"Training the model...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.view(images.size(0), -1).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Testing loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.view(images.size(0), -1).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jviX8hyUbMLX",
        "outputId": "139600f3-a38e-47e2-e1e9-70cf4a7194f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Training with ReLU activation function\n",
            "Epoch [1/5], Loss: 1.6358\n",
            "Epoch [2/5], Loss: 1.4281\n",
            "Epoch [3/5], Loss: 1.3250\n",
            "Epoch [4/5], Loss: 1.2450\n",
            "Epoch [5/5], Loss: 1.1734\n",
            "Test Accuracy: 51.93%, Test Loss: 1.3834, Training Time: 101.82 seconds\n",
            "\n",
            "Training with Tanh activation function\n",
            "Epoch [1/5], Loss: 1.7558\n",
            "Epoch [2/5], Loss: 1.6209\n",
            "Epoch [3/5], Loss: 1.5509\n",
            "Epoch [4/5], Loss: 1.5000\n",
            "Epoch [5/5], Loss: 1.4535\n",
            "Test Accuracy: 46.40%, Test Loss: 1.5276, Training Time: 100.80 seconds\n",
            "\n",
            "Training with Sigmoid activation function\n",
            "Epoch [1/5], Loss: 1.7876\n",
            "Epoch [2/5], Loss: 1.5937\n",
            "Epoch [3/5], Loss: 1.5006\n",
            "Epoch [4/5], Loss: 1.4272\n",
            "Epoch [5/5], Loss: 1.3691\n",
            "Test Accuracy: 48.33%, Test Loss: 1.4609, Training Time: 100.36 seconds\n",
            "\n",
            "Comparison of Activation Functions:\n",
            "ReLU: Accuracy = 51.93%, Loss = 1.3834, Training Time = 101.82 seconds\n",
            "Tanh: Accuracy = 46.40%, Loss = 1.5276, Training Time = 100.80 seconds\n",
            "Sigmoid: Accuracy = 48.33%, Loss = 1.4609, Training Time = 100.36 seconds\n",
            "\n",
            "Explanation:\n",
            "ReLU performs better for deep networks because it does not vanish gradient problem, allowing gradients to flow effectively. Tanh and Sigmoid can struggle due to saturating outputs, which diminish gradient updates in deeper layers. However, they may still perform well in shallower networks or specific tasks.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(in_features, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 32 * 32 * 3  # CIFAR-10 images are 32x32x3\n",
        "hidden_sizes = [256, 128]\n",
        "output_size = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Train and evaluate model with different activation functions\n",
        "activation_functions = {\n",
        "    \"ReLU\": nn.ReLU(),\n",
        "    \"Tanh\": nn.Tanh(),\n",
        "    \"Sigmoid\": nn.Sigmoid()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, activation_fn in activation_functions.items():\n",
        "    print(f\"\\nTraining with {name} activation function\")\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = MLP(input_size, hidden_sizes, output_size, activation_fn).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)  # Flatten images\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {test_loss:.4f}, Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"loss\": test_loss,\n",
        "        \"training_time\": training_time\n",
        "    }\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nComparison of Activation Functions:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: Accuracy = {metrics['accuracy']:.2f}%, Loss = {metrics['loss']:.4f}, Training Time = {metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "# Explanation:\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"ReLU performs better for deep networks because it does not vanish gradient problem, allowing gradients to flow effectively. Tanh and Sigmoid can struggle due to saturating outputs, which diminish gradient updates in deeper layers. However, they may still perform well in shallower networks or specific tasks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysoxw6ombNEB",
        "outputId": "da096ff8-06bb-4b6d-f3e5-245832f756dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=32, hidden_layers=[256, 128]\n",
            "Epoch [1/5], Loss: 1.6416\n",
            "Epoch [2/5], Loss: 1.4461\n",
            "Epoch [3/5], Loss: 1.3472\n",
            "Epoch [4/5], Loss: 1.2666\n",
            "Epoch [5/5], Loss: 1.1965\n",
            "Test Accuracy: 50.44%, Test Loss: 1.4295, Training Time: 124.60 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=32, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/5], Loss: 1.6662\n",
            "Epoch [2/5], Loss: 1.4597\n",
            "Epoch [3/5], Loss: 1.3475\n",
            "Epoch [4/5], Loss: 1.2607\n",
            "Epoch [5/5], Loss: 1.1794\n",
            "Test Accuracy: 51.58%, Test Loss: 1.3844, Training Time: 190.77 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=64, hidden_layers=[256, 128]\n",
            "Epoch [1/5], Loss: 1.6399\n",
            "Epoch [2/5], Loss: 1.4314\n",
            "Epoch [3/5], Loss: 1.3238\n",
            "Epoch [4/5], Loss: 1.2451\n",
            "Epoch [5/5], Loss: 1.1716\n",
            "Test Accuracy: 52.47%, Test Loss: 1.3881, Training Time: 99.93 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=64, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/5], Loss: 1.6617\n",
            "Epoch [2/5], Loss: 1.4438\n",
            "Epoch [3/5], Loss: 1.3227\n",
            "Epoch [4/5], Loss: 1.2311\n",
            "Epoch [5/5], Loss: 1.1490\n",
            "Test Accuracy: 52.60%, Test Loss: 1.3694, Training Time: 138.31 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=32, hidden_layers=[256, 128]\n",
            "Epoch [1/5], Loss: 2.1747\n",
            "Epoch [2/5], Loss: 2.1381\n",
            "Epoch [3/5], Loss: 2.1370\n",
            "Epoch [4/5], Loss: 2.1266\n",
            "Epoch [5/5], Loss: 2.1213\n",
            "Test Accuracy: 15.90%, Test Loss: 2.3292, Training Time: 126.52 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=32, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/5], Loss: 2.0934\n",
            "Epoch [2/5], Loss: 2.0265\n",
            "Epoch [3/5], Loss: 2.1413\n",
            "Epoch [4/5], Loss: 2.1708\n",
            "Epoch [5/5], Loss: 2.1871\n",
            "Test Accuracy: 15.59%, Test Loss: 2.1975, Training Time: 211.49 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=64, hidden_layers=[256, 128]\n",
            "Epoch [1/5], Loss: 1.9879\n",
            "Epoch [2/5], Loss: 1.9495\n",
            "Epoch [3/5], Loss: 1.9329\n",
            "Epoch [4/5], Loss: 1.9195\n",
            "Epoch [5/5], Loss: 1.9227\n",
            "Test Accuracy: 30.80%, Test Loss: 1.8874, Training Time: 102.05 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=64, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/5], Loss: 1.9597\n",
            "Epoch [2/5], Loss: 1.9402\n",
            "Epoch [3/5], Loss: 2.0639\n",
            "Epoch [4/5], Loss: 2.1668\n",
            "Epoch [5/5], Loss: 2.1755\n",
            "Test Accuracy: 16.33%, Test Loss: 2.1776, Training Time: 143.86 seconds\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "LR=0.001, Batch Size=32, Hidden Layers=(256, 128): Accuracy = 50.44%, Loss = 1.4295, Training Time = 124.60 seconds\n",
            "LR=0.001, Batch Size=32, Hidden Layers=(512, 256, 128): Accuracy = 51.58%, Loss = 1.3844, Training Time = 190.77 seconds\n",
            "LR=0.001, Batch Size=64, Hidden Layers=(256, 128): Accuracy = 52.47%, Loss = 1.3881, Training Time = 99.93 seconds\n",
            "LR=0.001, Batch Size=64, Hidden Layers=(512, 256, 128): Accuracy = 52.60%, Loss = 1.3694, Training Time = 138.31 seconds\n",
            "LR=0.01, Batch Size=32, Hidden Layers=(256, 128): Accuracy = 15.90%, Loss = 2.3292, Training Time = 126.52 seconds\n",
            "LR=0.01, Batch Size=32, Hidden Layers=(512, 256, 128): Accuracy = 15.59%, Loss = 2.1975, Training Time = 211.49 seconds\n",
            "LR=0.01, Batch Size=64, Hidden Layers=(256, 128): Accuracy = 30.80%, Loss = 1.8874, Training Time = 102.05 seconds\n",
            "LR=0.01, Batch Size=64, Hidden Layers=(512, 256, 128): Accuracy = 16.33%, Loss = 2.1776, Training Time = 143.86 seconds\n",
            "\n",
            "Discussion:\n",
            "1. Learning Rate:\n",
            "   - A lower learning rate (e.g., 0.001) leads to slower but more stable convergence, while a higher learning rate (e.g., 0.01) may speed up training but risks overshooting the optimal weights.\n",
            "2. Batch Size:\n",
            "   - Smaller batch sizes (e.g., 32) may lead to noisier gradient updates but can generalize better. Larger batch sizes (e.g., 64) stabilize training but may require more memory.\n",
            "3. Hidden Layers:\n",
            "   - Deeper networks (e.g., [512, 256, 128]) have more capacity to learn complex patterns but are prone to overfitting and require more training time. Simpler configurations (e.g., [256, 128]) may train faster but might underfit.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "from itertools import product\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(in_features, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Hyperparameter tuning setup\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [32, 64]\n",
        "hidden_layer_configs = [[256, 128], [512, 256, 128]]\n",
        "epochs = 5\n",
        "activation_fn = nn.ReLU()\n",
        "\n",
        "results = {}\n",
        "\n",
        "for lr, batch_size, hidden_layers in product(learning_rates, batch_sizes, hidden_layer_configs):\n",
        "    print(f\"\\nTraining with learning_rate={lr}, batch_size={batch_size}, hidden_layers={hidden_layers}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = MLP(32 * 32 * 3, hidden_layers, 10, activation_fn).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)  # Flatten images\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {test_loss:.4f}, Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Store results\n",
        "    results[(lr, batch_size, tuple(hidden_layers))] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"loss\": test_loss,\n",
        "        \"training_time\": training_time\n",
        "    }\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for (lr, batch_size, hidden_layers), metrics in results.items():\n",
        "    print(f\"LR={lr}, Batch Size={batch_size}, Hidden Layers={hidden_layers}: Accuracy = {metrics['accuracy']:.2f}%, Loss = {metrics['loss']:.4f}, Training Time = {metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "# Discussion on hyperparameters\n",
        "print(\"\\nDiscussion:\")\n",
        "print(\"1. Learning Rate:\")\n",
        "print(\"   - A lower learning rate (e.g., 0.001) leads to slower but more stable convergence, while a higher learning rate (e.g., 0.01) may speed up training but risks overshooting the optimal weights.\")\n",
        "print(\"2. Batch Size:\")\n",
        "print(\"   - Smaller batch sizes (e.g., 32) may lead to noisier gradient updates but can generalize better. Larger batch sizes (e.g., 64) stabilize training but may require more memory.\")\n",
        "print(\"3. Hidden Layers:\")\n",
        "print(\"   - Deeper networks (e.g., [512, 256, 128]) have more capacity to learn complex patterns but are prone to overfitting and require more training time. Simpler configurations (e.g., [256, 128]) may train faster but might underfit.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTFbBqsOdR0v",
        "outputId": "64521581-b9eb-47fa-e946-47a278efbe32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:11<00:00, 14.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=32, hidden_layers=[256, 128]\n",
            "Epoch [1/10], Loss: 3.7803\n",
            "Epoch [2/10], Loss: 3.4147\n",
            "Epoch [3/10], Loss: 3.2486\n",
            "Epoch [4/10], Loss: 3.1222\n",
            "Epoch [5/10], Loss: 3.0228\n",
            "Epoch [6/10], Loss: 2.9316\n",
            "Epoch [7/10], Loss: 2.8463\n",
            "Epoch [8/10], Loss: 2.7683\n",
            "Epoch [9/10], Loss: 2.6940\n",
            "Epoch [10/10], Loss: 2.6203\n",
            "Test Accuracy: 22.58%, Test Loss: 3.5116, Training Time: 257.83 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=32, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/10], Loss: 3.8164\n",
            "Epoch [2/10], Loss: 3.4576\n",
            "Epoch [3/10], Loss: 3.2854\n",
            "Epoch [4/10], Loss: 3.1602\n",
            "Epoch [5/10], Loss: 3.0527\n",
            "Epoch [6/10], Loss: 2.9650\n",
            "Epoch [7/10], Loss: 2.8706\n",
            "Epoch [8/10], Loss: 2.7917\n",
            "Epoch [9/10], Loss: 2.7076\n",
            "Epoch [10/10], Loss: 2.6269\n",
            "Test Accuracy: 21.96%, Test Loss: 3.5076, Training Time: 386.55 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=64, hidden_layers=[256, 128]\n",
            "Epoch [1/10], Loss: 3.7572\n",
            "Epoch [2/10], Loss: 3.3561\n",
            "Epoch [3/10], Loss: 3.1705\n",
            "Epoch [4/10], Loss: 3.0271\n",
            "Epoch [5/10], Loss: 2.9063\n",
            "Epoch [6/10], Loss: 2.8012\n",
            "Epoch [7/10], Loss: 2.7011\n",
            "Epoch [8/10], Loss: 2.6085\n",
            "Epoch [9/10], Loss: 2.5210\n",
            "Epoch [10/10], Loss: 2.4388\n",
            "Test Accuracy: 23.46%, Test Loss: 3.3966, Training Time: 202.19 seconds\n",
            "\n",
            "Training with learning_rate=0.001, batch_size=64, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/10], Loss: 3.8146\n",
            "Epoch [2/10], Loss: 3.4159\n",
            "Epoch [3/10], Loss: 3.2198\n",
            "Epoch [4/10], Loss: 3.0682\n",
            "Epoch [5/10], Loss: 2.9285\n",
            "Epoch [6/10], Loss: 2.8050\n",
            "Epoch [7/10], Loss: 2.6875\n",
            "Epoch [8/10], Loss: 2.5743\n",
            "Epoch [9/10], Loss: 2.4566\n",
            "Epoch [10/10], Loss: 2.3510\n",
            "Test Accuracy: 24.41%, Test Loss: 3.4829, Training Time: 283.75 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=32, hidden_layers=[256, 128]\n",
            "Epoch [1/10], Loss: 4.6154\n",
            "Epoch [2/10], Loss: 4.6104\n",
            "Epoch [3/10], Loss: 4.6106\n",
            "Epoch [4/10], Loss: 4.6100\n",
            "Epoch [5/10], Loss: 4.6104\n",
            "Epoch [6/10], Loss: 4.6107\n",
            "Epoch [7/10], Loss: 4.6107\n",
            "Epoch [8/10], Loss: 4.6103\n",
            "Epoch [9/10], Loss: 4.6106\n",
            "Epoch [10/10], Loss: 4.6103\n",
            "Test Accuracy: 1.00%, Test Loss: 4.6086, Training Time: 415.35 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=32, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/10], Loss: 4.6139\n",
            "Epoch [2/10], Loss: 4.6105\n",
            "Epoch [3/10], Loss: 4.6106\n",
            "Epoch [4/10], Loss: 4.6103\n",
            "Epoch [5/10], Loss: 4.6104\n",
            "Epoch [6/10], Loss: 4.6104\n",
            "Epoch [7/10], Loss: 4.6104\n",
            "Epoch [8/10], Loss: 4.6104\n",
            "Epoch [9/10], Loss: 4.6105\n",
            "Epoch [10/10], Loss: 4.6108\n",
            "Test Accuracy: 1.00%, Test Loss: 4.6080, Training Time: 722.02 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=64, hidden_layers=[256, 128]\n",
            "Epoch [1/10], Loss: 4.6159\n",
            "Epoch [2/10], Loss: 4.6089\n",
            "Epoch [3/10], Loss: 4.6092\n",
            "Epoch [4/10], Loss: 4.6091\n",
            "Epoch [5/10], Loss: 4.6092\n",
            "Epoch [6/10], Loss: 4.6091\n",
            "Epoch [7/10], Loss: 4.6091\n",
            "Epoch [8/10], Loss: 4.6091\n",
            "Epoch [9/10], Loss: 4.6092\n",
            "Epoch [10/10], Loss: 4.6092\n",
            "Test Accuracy: 1.00%, Test Loss: 4.6072, Training Time: 283.16 seconds\n",
            "\n",
            "Training with learning_rate=0.01, batch_size=64, hidden_layers=[512, 256, 128]\n",
            "Epoch [1/10], Loss: 4.4476\n",
            "Epoch [2/10], Loss: 4.5113\n",
            "Epoch [3/10], Loss: 4.5501\n",
            "Epoch [4/10], Loss: 4.5498\n",
            "Epoch [5/10], Loss: 4.5429\n",
            "Epoch [6/10], Loss: 4.5463\n",
            "Epoch [7/10], Loss: 4.5513\n",
            "Epoch [8/10], Loss: 4.5535\n",
            "Epoch [9/10], Loss: 4.5986\n",
            "Epoch [10/10], Loss: 4.5888\n",
            "Test Accuracy: 1.21%, Test Loss: 4.5921, Training Time: 333.43 seconds\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "LR=0.001, Batch Size=32, Hidden Layers=(256, 128): Accuracy = 22.58%, Loss = 3.5116, Training Time = 257.83 seconds\n",
            "LR=0.001, Batch Size=32, Hidden Layers=(512, 256, 128): Accuracy = 21.96%, Loss = 3.5076, Training Time = 386.55 seconds\n",
            "LR=0.001, Batch Size=64, Hidden Layers=(256, 128): Accuracy = 23.46%, Loss = 3.3966, Training Time = 202.19 seconds\n",
            "LR=0.001, Batch Size=64, Hidden Layers=(512, 256, 128): Accuracy = 24.41%, Loss = 3.4829, Training Time = 283.75 seconds\n",
            "LR=0.01, Batch Size=32, Hidden Layers=(256, 128): Accuracy = 1.00%, Loss = 4.6086, Training Time = 415.35 seconds\n",
            "LR=0.01, Batch Size=32, Hidden Layers=(512, 256, 128): Accuracy = 1.00%, Loss = 4.6080, Training Time = 722.02 seconds\n",
            "LR=0.01, Batch Size=64, Hidden Layers=(256, 128): Accuracy = 1.00%, Loss = 4.6072, Training Time = 283.16 seconds\n",
            "LR=0.01, Batch Size=64, Hidden Layers=(512, 256, 128): Accuracy = 1.21%, Loss = 4.5921, Training Time = 333.43 seconds\n",
            "\n",
            "Discussion:\n",
            "MLPs struggle with complex image datasets like CIFAR-100 for several reasons:\n",
            "1. Lack of Spatial Hierarchy: Unlike CNNs, MLPs do not exploit the spatial structure of images, leading to inefficient learning of features.\n",
            "2. High Dimensionality: Flattening images results in a loss of spatial relationships between pixels, which are crucial for understanding image content.\n",
            "3. Overfitting: MLPs require more parameters to handle high-dimensional data, increasing the risk of overfitting.\n",
            "4. Computational Inefficiency: MLPs with many parameters are computationally expensive and require significant resources for training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "from itertools import product\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(in_features, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR100(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Hyperparameter tuning setup\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [32, 64]\n",
        "hidden_layer_configs = [[256, 128], [512, 256, 128]]\n",
        "epochs = 10\n",
        "activation_fn = nn.ReLU()\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for lr, batch_size, hidden_layers in product(learning_rates, batch_sizes, hidden_layer_configs):\n",
        "    print(f\"\\nTraining with learning_rate={lr}, batch_size={batch_size}, hidden_layers={hidden_layers}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = MLP(32 * 32 * 3, hidden_layers, 100, activation_fn).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)  # Flatten images\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(images.size(0), -1).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {test_loss:.4f}, Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Store results\n",
        "    results[(lr, batch_size, tuple(hidden_layers))] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"loss\": test_loss,\n",
        "        \"training_time\": training_time\n",
        "    }\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for (lr, batch_size, hidden_layers), metrics in results.items():\n",
        "    print(f\"LR={lr}, Batch Size={batch_size}, Hidden Layers={hidden_layers}: Accuracy = {metrics['accuracy']:.2f}%, Loss = {metrics['loss']:.4f}, Training Time = {metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "# Explanation on MLPs and CIFAR-100\n",
        "print(\"\\nDiscussion:\")\n",
        "print(\"MLPs struggle with complex image datasets like CIFAR-100 for several reasons:\")\n",
        "print(\"1. Lack of Spatial Hierarchy: Unlike CNNs, MLPs do not exploit the spatial structure of images, leading to inefficient learning of features.\")\n",
        "print(\"2. High Dimensionality: Flattening images results in a loss of spatial relationships between pixels.\")\n",
        "print(\"3. Overfitting: MLPs require more parameters to handle high-dimensional data, increasing the risk of overfitting.\")\n",
        "print(\"4. Computational Inefficiency: MLPs with many parameters are computationally expensive and require significant resources for training.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}